<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>PureVoice AI</title>
  <style>
    html, body {
      margin: 0; padding: 0;
      width: 100%; height: 100%;
      display: flex; align-items: center; justify-content: center;
      background: #fff;
      font-family: sans-serif;
      color: #000;
    }
    #container { text-align: center; }
    #record-btn {
      width: 60px; height: 60px;
      border: none; border-radius: 50%;
      background: #444 url('/static/microphone-grey.svg') center/50% no-repeat;
      cursor: pointer; transition: box-shadow .3s;
    }
    #record-btn.disabled {
      cursor: not-allowed; opacity: 0.5;
    }
    #record-btn.recording {
      animation: pulse 1.5s ease-in-out infinite;
    }
    @keyframes pulse {
      0% { box-shadow: 0 0 0 0 rgba(0,0,0,0.2); }
      70% { box-shadow: 0 0 0 20px rgba(0,0,0,0); }
      100% { box-shadow: 0 0 0 0 rgba(0,0,0,0); }
    }
    #waveform, #response-waveform {
      width: 100px; height: 20px;
      margin: 10px auto;
      background: #fff;
    }
    #cta-text {
      margin-top: 10px; font-size: 1rem;
    }
    #transcript {
      margin-top: 20px; font-size: 0.9rem;
      color: #333; min-height: 1.2em;
    }
    #speaker-icon {
      width: 40px; height: 40px;
      margin: 20px auto 0;
      display: none;
    }
    #response-text {
      margin-top: 10px; font-size: 0.9rem;
      min-height: 1.2em;
    }
  </style>
</head>
<body>
  <div id="container">
    <button id="record-btn" aria-label="Record" class="disabled" disabled></button>
    <canvas id="waveform"></canvas>
    <p id="cta-text">Press to Speak</p>
    <div id="transcript"></div>
    <div id="speaker-icon">
      <svg viewBox="0 0 24 24" fill="none" stroke="#000" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
        <polygon points="11 5 6 9 2 9 2 15 6 15 11 19 11 5"></polygon>
        <path d="M19 5c.77.77 1.5 2.5 1.5 4s-.73 3.23-1.5 4"></path>
        <path d="M15 9c.5.5 1 1.5 1 3s-.5 2.5-1 3"></path>
      </svg>
    </div>
    <canvas id="response-waveform"></canvas>
    <p id="response-text"></p>
  </div>
  <script>
    const btn = document.getElementById('record-btn');
    const cta = document.getElementById('cta-text');
    const transcriptEl = document.getElementById('transcript');
    const speakerIcon = document.getElementById('speaker-icon');
    const responseWaveform = document.getElementById('response-waveform');
    const responseText = document.getElementById('response-text');
    const waveformCanvas = document.getElementById('waveform');
    let audioCtx, analyser, dataArray, animationId;
    let micStream, recorder, chunks;
    let isRecording = false;

    async function init() {
      try {
        micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        btn.disabled = false;
        btn.classList.remove('disabled');
        setupAnalyser();
      } catch (err) {
        cta.textContent = 'Microphone access denied';
      }
    }

    function setupAnalyser() {
      audioCtx = new (window.AudioContext || window.webkitAudioContext)();
      const source = audioCtx.createMediaStreamSource(micStream);
      analyser = audioCtx.createAnalyser(); analyser.fftSize = 256;
      dataArray = new Uint8Array(analyser.frequencyBinCount);
      source.connect(analyser);
      drawInputWaveform();
    }

    function drawInputWaveform() {
      cancelAnimationFrame(animationId);
      const canvas = waveformCanvas;
      const ctx = canvas.getContext('2d');
      canvas.width = canvas.clientWidth;
      canvas.height = canvas.clientHeight;
      function draw() {
        animationId = requestAnimationFrame(draw);
        analyser.getByteTimeDomainData(dataArray);
        ctx.fillStyle = '#fff'; ctx.fillRect(0, 0, canvas.width, canvas.height);
        ctx.lineWidth = 2; ctx.strokeStyle = '#444'; ctx.beginPath();
        const sliceW = canvas.width / dataArray.length;
        let x = 0;
        for (let i = 0; i < dataArray.length; i++) {
          const v = dataArray[i] / 128.0;
          const y = v * (canvas.height / 2);
          if (i === 0) ctx.moveTo(x, y); else ctx.lineTo(x, y);
          x += sliceW;
        }
        ctx.lineTo(canvas.width, canvas.height/2);
        ctx.stroke();
      }
      draw();
    }

    btn.addEventListener('click', () => {
      if (!isRecording) startRecording(); else stopRecording();
    });

    function startRecording() {
      isRecording = true; btn.classList.add('recording'); cta.textContent = 'Press to Stop';
      chunks = [];
      recorder = new MediaRecorder(micStream, { mimeType: 'audio/webm' });
      recorder.ondataavailable = e => chunks.push(e.data);
      recorder.start();
    }

    function stopRecording() {
      isRecording = false; btn.classList.remove('recording'); cta.textContent = 'Processing...';
      recorder.onstop = onRecordingStop; recorder.stop(); cancelAnimationFrame(animationId);
    }

    async function onRecordingStop() {
      const blob = new Blob(chunks, { type: 'audio/webm' });
      const form = new FormData(); form.append('audio', blob, 'input.webm');
      transcriptEl.textContent = 'Transcribing...';
      try {
        const res = await fetch('/process', { method: 'POST', body: form });
        const json = await res.json();
        transcriptEl.textContent = json.transcript;
        playResponse(json.audio_base64, json.voice_name, json.response_text);
      } catch {
        transcriptEl.textContent = 'Processing error';
      } finally {
        cta.textContent = 'Press to Speak';
      }
    }

    function playResponse(b64, voiceName, respText) {
      speakerIcon.style.display = 'block'; responseWaveform.style.display = 'block'; responseText.textContent = '';
      const audio = new Audio('data:audio/mp3;base64,' + b64);
      audio.onplay = () => { responseText.textContent = voiceName + ' is speaking'; drawResponseWaveform(audio); };
      audio.onended = () => { speakerIcon.style.display = 'none'; responseWaveform.style.display = 'none'; responseText.textContent = respText; };
      audio.play();
    }

    function drawResponseWaveform(audio) {
      const respCtx = new (window.AudioContext || window.webkitAudioContext)();
      const source = respCtx.createMediaElementSource(audio);
      const respAnalyser = respCtx.createAnalyser(); respAnalyser.fftSize = 256;
      const dataArr = new Uint8Array(respAnalyser.frequencyBinCount);
      source.connect(respAnalyser); respAnalyser.connect(respCtx.destination);
      const canvas = responseWaveform; const ctx = canvas.getContext('2d');
      canvas.width = canvas.clientWidth; canvas.height = canvas.clientHeight;
      function draw() {
        if (audio.paused) return;
        requestAnimationFrame(draw);
        respAnalyser.getByteTimeDomainData(dataArr);
        ctx.fillStyle = '#fff'; ctx.fillRect(0, 0, canvas.width, canvas.height);
        ctx.lineWidth = 1; ctx.strokeStyle = '#000'; ctx.beginPath();
        const sliceW = canvas.width / dataArr.length; let x = 0;
        for (let i = 0; i < dataArr.length; i++) {
          const v = dataArr[i] / 128.0; const y = v * (canvas.height / 2);
          if (i === 0) ctx.moveTo(x, y); else ctx.lineTo(x, y);
          x += sliceW;
        }
        ctx.stroke();
      }
      draw();
    }
    init();
  </script>
</body>
</html>
